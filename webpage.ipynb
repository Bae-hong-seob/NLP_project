{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "odg3vDEV8QNp"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Zcw2zoc28HLF"
   },
   "outputs": [],
   "source": [
    "# 파이캐럿 사용\n",
    "#!pip install pycaret\n",
    "#!pip install Jinja2\n",
    "#!pip install markupsafe==2.1.1\n",
    "#!pip install xgboost\n",
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J3vavATW8Je0"
   },
   "outputs": [],
   "source": [
    "#file_path = '/content/drive/MyDrive/Text_Mining/'\n",
    "file_path = ''\n",
    "file_name_01 = 'final_total_df_01_res_20.csv'\n",
    "file_name_03 = 'final_total_df_03_res_22.csv'\n",
    "file_name_06 = 'final_total_df_06_18.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시현을 위한 배포이므로 \n",
    "    1. 비교적 비용이 낮은 레스토랑\n",
    "    2. 비교적 비용이 높은 레스토랑\n",
    "    3. 보트 관련 업체\n",
    "    4. 서적 관련 업체\n",
    "    5. 호텔 \n",
    "    \n",
    "- 가게를 사용해 구현 -> 필요한 데이터셋 3개만 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dZx-uMBn8w8l",
    "outputId": "2b360a9e-9931-4303-d6ab-01f47c5a9578"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_676/1830159039.py:4: DtypeWarning: Columns (17,25,27,30,32,33,35,36,37,39,41,45,51,52,57,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  total_df_01 = pd.read_csv(file_path + file_name_01)\n",
      "/var/tmp/ipykernel_676/1830159039.py:5: DtypeWarning: Columns (25,33,36,43,58,64,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  total_df_03 = pd.read_csv(file_path + file_name_03)\n",
      "/var/tmp/ipykernel_676/1830159039.py:6: DtypeWarning: Columns (25,28,29,32,34,36,44,46,47,50,64,65,66,67,68,69,70,71,73,74) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  total_df_06 = pd.read_csv(file_path + file_name_06)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "total_df_01 = pd.read_csv(file_path + file_name_01)\n",
    "total_df_03 = pd.read_csv(file_path + file_name_03)\n",
    "total_df_06 = pd.read_csv(file_path + file_name_06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WA9Vguwa1FIH"
   },
   "source": [
    "## 준비 : dataset으로 샘플 만들기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QKBm5pQ09VzM"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    total_df_01 = total_df_01.drop('Unnamed: 0', axis=1)\n",
    "    total_df_03 = total_df_03.drop('Unnamed: 0', axis=1)\n",
    "    total_df_06 = total_df_06.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "except:\n",
    "    print('No index column')\n",
    "\n",
    "final_df_01 = total_df_01.drop('text', axis=1)\n",
    "final_df_03 = total_df_03.drop('text', axis=1)\n",
    "final_df_06 = total_df_06.drop('text', axis=1)\n",
    "\n",
    "category = 'summary'\n",
    "\n",
    "remove_col = ['review_id','user_id','business_id', category, 'date','name','is_open']\n",
    "# remove_col = ['review_id','user_id','business_id','date','name','is_open']\n",
    "\n",
    "\n",
    "final_df_01 = final_df_01.drop(remove_col, axis = 1)\n",
    "\n",
    "final_df_01 = final_df_01.replace(False, 0)\n",
    "final_df_01 = final_df_01.replace('False', 0)\n",
    "final_df_01 = final_df_01.replace(True, 1)\n",
    "final_df_01 = final_df_01.replace('True', 1)\n",
    "\n",
    "# Identify object type columns\n",
    "object_columns = final_df_01.select_dtypes(include='object').columns\n",
    "\n",
    "# Convert object columns to numeric or categorical\n",
    "for column in object_columns:\n",
    "    try:\n",
    "        final_df_01[column] = pd.to_numeric(final_df_01[column], errors='coerce')\n",
    "    except ValueError:\n",
    "        final_df_01[column] = final_df_01[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_03 = final_df_03.drop(remove_col, axis = 1)\n",
    "\n",
    "final_df_03 = final_df_03.replace(False, 0)\n",
    "final_df_03 = final_df_03.replace('False', 0)\n",
    "final_df_03 = final_df_03.replace(True, 1)\n",
    "final_df_03 = final_df_03.replace('True', 1)\n",
    "\n",
    "# Identify object type columns\n",
    "object_columns = final_df_03.select_dtypes(include='object').columns\n",
    "\n",
    "# Convert object columns to numeric or categorical\n",
    "for column in object_columns:\n",
    "    try:\n",
    "        final_df_03[column] = pd.to_numeric(final_df_03[column], errors='coerce')\n",
    "    except ValueError:\n",
    "        final_df_03[column] = final_df_03[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_06 = final_df_06.drop(remove_col, axis = 1)\n",
    "\n",
    "final_df_06 = final_df_06.replace(False, 0)\n",
    "final_df_06 = final_df_06.replace('False', 0)\n",
    "final_df_06 = final_df_06.replace(True, 1)\n",
    "final_df_06 = final_df_06.replace('True', 1)\n",
    "\n",
    "# Identify object type columns\n",
    "object_columns = final_df_06.select_dtypes(include='object').columns\n",
    "\n",
    "# Convert object columns to numeric or categorical\n",
    "for column in object_columns:\n",
    "    try:\n",
    "        final_df_06[column] = pd.to_numeric(final_df_06[column], errors='coerce')\n",
    "    except ValueError:\n",
    "        final_df_06[column] = final_df_06[column].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- user 정보를 위한 임의의 data 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v_7Rs3tRMUjX"
   },
   "outputs": [],
   "source": [
    "sample_data_01 = final_df_01.iloc[1566:1567]\n",
    "sample_data_03 = final_df_03.iloc[1567:1568]\n",
    "sample_data_06 = final_df_06.iloc[1566:1567]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gplLt1qGj5yj"
   },
   "source": [
    "# 1. summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "530q4YYsRKTa"
   },
   "outputs": [],
   "source": [
    "def summary(review):\n",
    "    # Create a temporary DataFrame\n",
    "    temp_df = pd.DataFrame([str(review)], columns=['text'])\n",
    "\n",
    "    # Calculate word count\n",
    "    temp_df['word_count'] = temp_df['text'].str.split().str.len()\n",
    "\n",
    "    # Filter by word count\n",
    "    temp_df = temp_df[temp_df['word_count'] <= 300]\n",
    "\n",
    "    # If there is any text left after the filtering, convert the first one to string\n",
    "    if not temp_df.empty:\n",
    "        review_summary = temp_df['text'].iloc[0]\n",
    "            \n",
    "    else:\n",
    "        print(\"입력 된게 없어요!!\")\n",
    "\n",
    "    return review_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9zH2oStkEQM"
   },
   "source": [
    "# 2. text 전처리 & sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nNJ7ztXlXoGF"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJJX0NbDcntA",
    "outputId": "2e91195d-2f20-470c-ba3e-bad78ebe6fd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/visualinformaticslab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/visualinformaticslab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/visualinformaticslab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/visualinformaticslab/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# 전처리에 필요한 라이브러리 import\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from transformers import pipeline\n",
    "from pycaret.regression import *\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "u7oR3tczcolf"
   },
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "    \"\"\"\n",
    "    split the document into sentences and tokenize each sentence\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self,text):\n",
    "        \"\"\"\n",
    "        out : ['What', 'can', 'I', 'say', 'about', 'this', 'place', '.']\n",
    "        \"\"\"\n",
    "        # split into single sentence\n",
    "        sentences = self.splitter.tokenize(text)\n",
    "        # tokenization in each sentences\n",
    "        tokens = [self.tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokens\n",
    "\n",
    "class LemmatizationWithPOSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_wordnet_pos(self,treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v)\n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def pos_tag(self,tokens):\n",
    "        # find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n",
    "        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n",
    "\n",
    "        # lemmatization using pos tagg\n",
    "        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n",
    "        pos_tokens = [ [(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)), [pos_tag]) for (word,pos_tag) in pos] for pos in pos_tokens]\n",
    "        return pos_tokens\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Initialize required classes and objects\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    splitter = Splitter()\n",
    "    lemmatization_using_pos_tagger = LemmatizationWithPOSTagger()\n",
    "\n",
    "    # Step 1: Split document into sentence followed by tokenization\n",
    "    tokens = splitter.split(text)\n",
    "\n",
    "    # Step 2: Lemmatization using pos tagger\n",
    "    lemma_pos_token = lemmatization_using_pos_tagger.pos_tag(tokens)\n",
    "\n",
    "    # Create lemmatized review\n",
    "    lemmatized_text = ' '.join(word[1] for words in lemma_pos_token for word in words)\n",
    "\n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fEuEuvGXfOgu"
   },
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "custom_stopwords = ['(', ')']\n",
    "\n",
    "#custom stopword 추가\n",
    "stop_words.update(custom_stopwords)\n",
    "stop_words\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Import NLTK stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    result = ''\n",
    "    word_tokens = word_tokenize(text)\n",
    "    word_tokens = pos_tag(word_tokens)\n",
    "\n",
    "    for word,tag in word_tokens:\n",
    "\n",
    "        # Convert tokens to lowercase\n",
    "        word = word.lower()\n",
    "\n",
    "        # Remove stop words\n",
    "        if word not in stop_words:\n",
    "            result += ' ' + word\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "x19JC2yqf25y"
   },
   "outputs": [],
   "source": [
    "def get_sentiment_score(text):\n",
    "    sentiment_score = None\n",
    "    # 경고문 emoji(이모티콘) 라이브러리 설치\n",
    "    # 우리 텍스트문에 이모티콘 있나 탐색해보고 있으면 설치\n",
    "\n",
    "\n",
    "    data = [\"I love you\", \"I hate you\"]\n",
    "\n",
    "    # bertweet_base_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\", device=0)\n",
    "    bertweet_base_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        sentiment = bertweet_base_model(text)[0]\n",
    "        label = sentiment['label']\n",
    "        score = sentiment['score']\n",
    "\n",
    "        # Map sentiment_value based on the label\n",
    "        if label == 'POS':\n",
    "            sentiment_score = score\n",
    "        elif label == 'NEG':\n",
    "            sentiment_score = 1 - score\n",
    "        else:\n",
    "            sentiment_score = 0.5\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred: \", str(e))\n",
    "\n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWKZSk_wkxX_"
   },
   "source": [
    "- 밑에 코드 실행하면 됨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f7zD79kakOF3"
   },
   "outputs": [],
   "source": [
    "def preprocess_and_sentiment(review_summary):\n",
    "    temp_text = lemmatize_text(review_summary)\n",
    "    temp_text = tokenize_text(temp_text)\n",
    "\n",
    "    sentiment_score = get_sentiment_score(temp_text)\n",
    "\n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_87IEj1l0SK"
   },
   "source": [
    "# 3. pycaret 모델 불러오기!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZcy6H8El244"
   },
   "source": [
    "- 일단 6번으로 가정하고! 진행하고 다 한다음에 여기 if로 해서 수정하면 됨!!\n",
    "    - if categories 해서 ㅇㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciq2zXEbk43j",
    "outputId": "15112eef-6076-4d2a-eca7-1a04a73f0d45"
   },
   "outputs": [],
   "source": [
    "#def choice_model():\n",
    "\n",
    "def pycaret_model(final_model,sample_data):\n",
    "\n",
    "    prediction = predict_model(final_model, data=sample_data)\n",
    "\n",
    "    user_mean = 3\n",
    "    user_var = 0.7\n",
    "    predict_zscore = prediction['prediction_label'].values[0]\n",
    "    # 리뷰에 대해 star(specific_stars)를 입력했을 때 상대적 중요도 z-score 표현 식.\n",
    "    z_score = (predict_zscore - user_mean) / np.sqrt(user_var)\n",
    "    \n",
    "    return z_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3zlz2yzp_R6"
   },
   "source": [
    "# 4. 웹으로 띄웁시다!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "eHGyk4eHARrh",
    "outputId": "c65cd82a-2563-41eb-eed7-2adc0f0f6e87",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://699702d059b54f2f73.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://699702d059b54f2f73.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "# 모델 함수 (여기에서는 예시로 단순한 로직을 사용하였습니다. 실제 모델 로직으로 변경해주세요)\n",
    "def calculate_tip(store, price, review):\n",
    "    # 입력: 리뷰 텍스트 (문자열), 음식 가격\n",
    "    # 출력: 팁 (문자열)\n",
    "    price = float(price)\n",
    "    \n",
    "    if len(review) <= 0:\n",
    "        return \"리뷰를 입력해주세요\"\n",
    "    \n",
    "    # 데이터 summary\n",
    "    summary_review = summary(review)\n",
    "    # 데이터 전처리&sentiment\n",
    "    handled_review = preprocess_and_sentiment(summary_review)    \n",
    "    \n",
    "\n",
    "    # 가게에 따라 \n",
    "    if store == \"4 Kids Books and Toys\":\n",
    "        \n",
    "        return \"팁을 주지 않으셔도 되지만, 좋은 경험이었다면 작은 성의를 표시하셔도 좋을 것 같습니다.\"\n",
    "    \n",
    "    elif store == \"Tsevi's Pub And Grill\":\n",
    "        \n",
    "        final_model_01 = load_model(file_path + 'model_final_total_df_01_res_20.csv')\n",
    "        sample_data_01['sentiment_score'] = [handled_review]\n",
    "\n",
    "        predict_zscore =  pycaret_model(final_model_01, sample_data_01)\n",
    "        defalut = 0.2  # <- 얘도 제어문 처리!\n",
    "\n",
    "    elif store == \"Mo's a Place For Steaks\":\n",
    "        final_model_03 = load_model(file_path + 'model_final_total_df_03_res_22.csv')\n",
    "        sample_data_03['sentiment_score'] = [handled_review]\n",
    "\n",
    "        predict_zscore =  pycaret_model(final_model_03, sample_data_03)\n",
    "        defalut = 0.22  # <- 얘도 제어문 처리!\n",
    "\n",
    "    elif store == \"Thriller Clearwater\":\n",
    "        final_model_06 = load_model(file_path + 'model_final_total_df_06_18.csv')\n",
    "        sample_data_06['sentiment_score'] = [handled_review]\n",
    "\n",
    "        predict_zscore =  pycaret_model(final_model_06, sample_data_06)\n",
    "        defalut = 0.18  # <- 얘도 제어문 처리!\n",
    "\n",
    "    elif store == \"Spring Mount Hotel\":\n",
    "        \n",
    "        return \"Housekeeping에게 2~3dollar, Valet에게 3~5 doaller, Bellman에게 1~2 dollar 정도로 팁을 주시는 것을 추천합니다\"\n",
    "    \n",
    "    else:\n",
    "        return \"가게를 선택하지 않았습니다.\"\n",
    "    \n",
    "    # 있다고 가정!\n",
    "\n",
    "    tip_stdev = 0.01\n",
    "\n",
    "    # 최종 팁 스코어 계산\n",
    "    tip_percent = defalut + predict_zscore * tip_stdev\n",
    "    \n",
    "    output_price = price * tip_percent\n",
    "    \n",
    "    output_price = int(output_price)\n",
    "    tip_percent = tip_percent*100\n",
    "    \n",
    "    tip_percent = \"{:.2f}\".format(tip_percent)\n",
    "    \n",
    "    output_tip_percent = \"\"\n",
    "    \n",
    "    return f\"{tip_percent}%로 팁을 주시는 것을 추천합니다. \\n\\n계산 결과 : ${output_price}\"\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=calculate_tip, # 위에서 정의한 함수\n",
    "    inputs=[\n",
    "        gr.inputs.Dropdown([\"4 Kids Books and Toys\", \"Tsevi's Pub And Grill\", \"Mo's a Place For Steaks\", \"Thriller Clearwater\", \"Spring Mount Hotel\"], label=\"방문할 가게를 선택해주세요:\"),\n",
    "        gr.inputs.Number(label=\"구매 가격을 입력해주세요:\"),\n",
    "        gr.inputs.Textbox(lines=5, placeholder=\"이 가게에 대한 리뷰를 여기에 입력해주세요...\")\n",
    "    ],\n",
    "    outputs=gr.outputs.Textbox(),\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
